# üìñ Documentaci√≥n T√©cnica - Ollama MCP Server

## üèóÔ∏è Arquitectura del Sistema

El Ollama MCP Server est√° dise√±ado como un sistema modular que prop```
mcp-servers/
‚îú‚îÄ‚îÄ ollama_mcp_server.py    # ‚úÖ Servidor principal interactivo
‚îú‚îÄ‚îÄ mcp_direct.py          # ‚úÖ Comandos directos
‚îú‚îÄ‚îÄ install.py             # ‚úÖ Instalador autom√°tico
‚îú‚îÄ‚îÄ requirements.txt       # ‚úÖ Dependencias m√≠nimas
‚îú‚îÄ‚îÄ .env                   # ‚úÖ Configuraci√≥n real (gitignore)
‚îú‚îÄ‚îÄ .env.example          # ‚úÖ Plantilla de configuraci√≥n
‚îú‚îÄ‚îÄ .bashrc.example       # ‚úÖ Configuraci√≥n terminal optimizada
‚îú‚îÄ‚îÄ README.md             # ‚úÖ Documentaci√≥n de usuario
‚îú‚îÄ‚îÄ DOCUMENTATION.md      # ‚úÖ Este archivo - documentaci√≥n t√©cnica
‚îî‚îÄ‚îÄ .venv/               # ‚úÖ Entorno virtual
```o a IA local y recursos de GitHub desde cualquier directorio del sistema.

### Componentes Principales

#### 1. **ollama_mcp_server.py** - Servidor Interactivo
- **Funci√≥n**: Servidor principal con interfaz de comandos interactiva
- **Uso**: `mcp start` -> Abre terminal interactivo `ü§ñ MCP>`
- **Modelos**: 
  - `llama3.1:8b` para chat general
  - `deepseek-coder:6.7b` para asistencia de c√≥digo
- **Funciones**:
  - Chat con modelos locales
  - B√∫squeda y an√°lisis de repositorios GitHub
  - An√°lisis y revisi√≥n de c√≥digo
  - Obtenci√≥n de archivos espec√≠ficos de repos

#### 2. **mcp_direct.py** - Comandos Directos
- **Funci√≥n**: Ejecutor de comandos directos sin interfaz interactiva
- **Uso**: `mcp chat "pregunta"`, `mcp code "consulta"`, etc.
- **Beneficio**: Permite usar el agente desde cualquier proyecto sin cambiar de directorio

#### 3. **install.py** - Instalador Autom√°tico
- **Funci√≥n**: Configuraci√≥n autom√°tica del entorno
- **Tareas**:
  - Verificar Ollama instalado
  - Crear entorno virtual
  - Instalar dependencias
  - Descargar modelos recomendados
  - Configurar aliases

### Flujo de Datos

```mermaid
graph TD
    A[Usuario] --> B{Tipo de comando}
    B -->|mcp start| C[ollama_mcp_server.py]
    B -->|mcp chat/code/github| D[mcp_direct.py]
    C --> E[Interfaz Interactiva]
    D --> F[Ejecuci√≥n Directa]
    E --> G[Ollama API]
    F --> G
    G --> H[Modelos Locales]
    C --> I[GitHub API]
    D --> I
    I --> J[Repositorios GitHub]
```

## üîß Configuraci√≥n del Sistema

### Variables de Entorno (.env)

```bash
# Token de GitHub (REQUERIDO)
GITHUB_TOKEN=tu_token_aqui

# Configuraci√≥n de Ollama (opcional)
OLLAMA_URL=http://localhost:11434
DEFAULT_MODEL=llama3.1:8b
CODING_MODEL=deepseek-coder:6.7b

# Configuraci√≥n del servidor (opcional)
SERVER_HOST=localhost
SERVER_PORT=8000
```

### Configuraci√≥n del Terminal (.bashrc)

El archivo `.bashrc.example` contiene:

1. **Funci√≥n principal `mcp()`**: Maneja todos los comandos
2. **Aliases directos**: `ai-chat`, `ai-code`, `ai-github`
3. **Comandos de an√°lisis**: `analyze`, `review`
4. **Gesti√≥n de modelos**: `mcp models`, `mcp pull`

## üöÄ Modos de Operaci√≥n

### Modo Interactivo (ollama_mcp_server.py)

**Ventajas**:
- Conversaci√≥n continua con contexto
- Interfaz completa con todos los comandos
- Ideal para sesiones largas de desarrollo

**Desventajas**:
- Requiere terminal dedicado
- Hay que estar en el directorio del proyecto

### Modo Directo (mcp_direct.py)

**Ventajas**:
- Acceso instant√°neo desde cualquier directorio
- Perfecto para consultas r√°pidas
- Se integra en workflows existentes

**Desventajas**:
- Sin contexto entre comandos
- Menos funciones interactivas

## üîç Funciones Principales

### Chat con IA Local
```python
def _ollama_request(self, model: str, prompt: str, system: str = "") -> str:
    """Solicitud HTTP a la API local de Ollama"""
    data = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    if system:
        data["system"] = system
    
    response = requests.post(f"{self.ollama_url}/api/generate", json=data)
    return response.json().get('response')
```

### Integraci√≥n con GitHub
```python
def github_search(self, query: str) -> str:
    """B√∫squeda de repositorios en GitHub"""
    headers = {'Authorization': f'token {self.github_token}'}
    url = "https://api.github.com/search/repositories"
    params = {'q': query, 'sort': 'stars', 'order': 'desc', 'per_page': 5}
    
    response = requests.get(url, headers=headers, params=params)
    return self._format_github_results(response.json())
```

### An√°lisis de C√≥digo
```python
def analyze_code(self, code: str) -> str:
    """An√°lisis profundo de c√≥digo con IA especializada"""
    system_prompt = """Eres un experto en an√°lisis de c√≥digo. Analiza el c√≥digo
    proporcionado y proporciona insights sobre:
    - Calidad y estructura
    - Posibles mejoras
    - Bugs potenciales
    - Mejores pr√°cticas"""
    
    return self._ollama_request(self.coding_model, code, system_prompt)
```

## üìÅ Estructura de Archivos

```
~/mcp-servers/
‚îú‚îÄ‚îÄ ollama_mcp_server.py    # Servidor principal interactivo
‚îú‚îÄ‚îÄ mcp_direct.py          # Comandos directos
‚îú‚îÄ‚îÄ install.py             # Instalador autom√°tico
‚îú‚îÄ‚îÄ requirements.txt       # Dependencias m√≠nimas
‚îú‚îÄ‚îÄ .env                   # Configuraci√≥n real (gitignore)
‚îú‚îÄ‚îÄ .env.example          # Plantilla de configuraci√≥n
‚îú‚îÄ‚îÄ .bashrc.example       # Configuraci√≥n terminal optimizada
‚îú‚îÄ‚îÄ README.md             # Documentaci√≥n de usuario
‚îú‚îÄ‚îÄ DOCUMENTATION.md      # Este archivo - documentaci√≥n t√©cnica
‚îî‚îÄ‚îÄ .venv/               # Entorno virtual
```

## üîê Seguridad y Privacidad

- **Todo local**: Los modelos de IA funcionan completamente offline
- **GitHub API**: Solo accede a repositorios p√∫blicos (o privados si tienes acceso)
- **Sin telemetr√≠a**: No se env√≠an datos a servicios externos
- **Tokens seguros**: GitHub token se almacena localmente en `.env`

## üõ†Ô∏è Desarrollo y Extensi√≥n

### Agregar Nuevos Comandos

1. **Servidor interactivo**: Agregar m√©todo a la clase `OllamaMCPServer`
2. **Comandos directos**: Agregar m√©todo a la clase `MCPDirect`
3. **Aliases**: Actualizar `.bashrc.example`

### Agregar Nuevos Modelos

```bash
# Instalar modelo
ollama pull nombre-del-modelo

# Usar en c√≥digo
model = "nombre-del-modelo"
response = self._ollama_request(model, prompt)
```

### Integrar Nuevas APIs

```python
# Ejemplo: integraci√≥n con API externa
def new_api_integration(self, query: str) -> str:
    headers = {'Authorization': f'Bearer {self.api_token}'}
    response = requests.get(f'https://api.example.com/search?q={query}', headers=headers)
    return self._process_response(response.json())
```

## üöÄ Optimizaci√≥n y Performance

### Recomendaciones

1. **Modelos eficientes**: 
   - `llama3.2:3b` para tareas ligeras
   - `qwen2.5-coder:7b` para c√≥digo (balance velocidad/calidad)
   - `deepseek-coder:6.7b` para an√°lisis profundo

2. **Cach√© de respuestas**: Implementar cach√© local para consultas repetidas

3. **Timeouts configurables**: Ajustar timeouts seg√∫n capacidad del hardware

4. **Procesamiento en paralelo**: Para m√∫ltiples consultas simult√°neas

### Monitoreo

```bash
# Ver uso de recursos de Ollama
ollama ps

# Monitorear logs del sistema
tail -f ~/.ollama/logs/server.log

# Estad√≠sticas de uso
mcp status
```

## üîÑ Actualizaciones y Mantenimiento

### Actualizar Modelos
```bash
ollama pull llama3.1:8b      # Actualizar modelo existente
mcp models                   # Ver versiones instaladas
```

### Actualizar Dependencias

```bash
cd mcp-servers
.venv/bin/pip install --upgrade requests python-dotenv PyGithub
```

### Backup de Configuraci√≥n
```bash
# Respaldar configuraci√≥n
cp .env .env.backup
cp ~/.bashrc ~/.bashrc.backup
```

## üìä Casos de Uso Avanzados

### Integraci√≥n en IDEs

El sistema se puede integrar f√°cilmente en IDEs como VSCode mediante extensiones que ejecuten los comandos directos.

### CI/CD Pipelines

Los comandos directos pueden usarse en scripts de CI/CD para an√°lisis autom√°tico de c√≥digo:

```bash
# En un pipeline
analyze src/main.py > code_analysis.txt
review src/tests/ > code_review.txt
```

### Desarrollo de Equipos

Cada desarrollador puede tener su propia configuraci√≥n mientras comparte los mismos scripts base.

---

*Esta documentaci√≥n est√° en constante evoluci√≥n. Para contribuir o reportar problemas, consulta el README principal.*
